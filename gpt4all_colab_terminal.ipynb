{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaykizzzle/GenerativeAI/blob/main/gpt4all_colab_terminal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set to GPU first!"
      ],
      "metadata": {
        "id": "XN1K1tVpyt_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U accelerate bitsandbytes transformers huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhTtJspyl0RG",
        "outputId": "6898ef08-da95-4a58-a181-8b3a9b49a32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.4)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "#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"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRni51iIuoQ9",
        "outputId": "b6b318a7-f331-4db2-9d11-520288bcb361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary classes for model loading and quantization\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Configure model quantization to 4-bit for memory and computation efficiency\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "#configure quantization by setting load_in_4bit=True, indicating that the model's\n",
        "#weights should be pushed in 4-bit precision instead of the original 32-bit.\n",
        "#This reduces memory consumption and potentially speeds up computations, making the\n",
        "#model more efficient for resource-constrained environments.\n",
        "\n",
        "\n",
        "# Load the tokenizer for the Gemma 7B Italian model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\n",
        "#load the pre-trained tokenizer specifically designed for the “google/gemma-7b-it”\n",
        "#model using AutoTokenizer.from_pretrained(“google/gemma-7b-it”).\n",
        "\n",
        "\n",
        "# Load the Gemma 7B Italian model itself, with 4-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\",\n",
        "                                             quantization_config=quantization_config)\n",
        "#load the actual “google/gemma-7b-it” model, but with the crucial addition\n",
        "#of the quantization configuration, ensuring that the model weights are created in the 4-bit format.\n",
        "\n",
        "\n",
        "#Gemma Large Language Model is downloaded, converted into a 4-bit quantized model, and loaded into the GPU.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "1cd2ded675f442d3bc71fc728fb4842c",
            "756b89517be441c499e14ff764854d03",
            "d86abe6fba874423bee1676cb369a276",
            "2f84caa194ef47e1ab7a07f70a83e20b",
            "f01430ac583145bc930db97b944d466c",
            "123c0d2fa9ae467ea9a5ea219e18a4ca",
            "6eb3a7d891064d94b8aeb9e4ee8f383f",
            "8f0892e1666c4041a12f34817e8ae9b3",
            "b07f43b0cab242ec9dcc5102913e8c63",
            "314e96af884049d9b4777c305cb785fc",
            "113250a32c2c4120b3282becc5d7c1a3"
          ]
        },
        "id": "wTcBWjtsvADs",
        "outputId": "5435c7ca-28aa-41bb-c187-3013cd36a360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
            "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1cd2ded675f442d3bc71fc728fb4842c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#start by setting the prompt\n",
        "input_text = \"who is joe\"\n",
        "\n",
        "# Now lets Tokenize the input text\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "#call the model's generate function with the tokenized input and set a maximum output length of 512 tokens.\n",
        "#This tells the model to generate text based on the given prompt while respecting the length limit.\n",
        "outputs = model.generate(\n",
        "    **input_ids,  # Pass tokenized input as keyword argument\n",
        "    max_length=512,  # Limit output length to 512 tokens\n",
        ")\n",
        "\n",
        "# The generated text, represented as a sequence of token IDs, is decoded back into human-readable text using the tokenizer.decode function.\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EpN8_qVxjFf",
        "outputId": "e3e51873-afe8-44b7-ca42-3ae8ac6047b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos>who is joe biden?\n",
            "\n",
            "Joe Biden is the current President of the United States of America. Born in 1942, he is the oldest president in U.S. history. A Democrat, he served as Vice President under Barack Obama from 2009 to 2012 and won the presidency in 2020.<eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fine Tuning Gemma to reflect our datasets!"
      ],
      "metadata": {
        "id": "wq1Wsjr_zsJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q -U datasets\n",
        "!pip3 install -q -U pyarrow==15.0.0\n",
        "!pip install --upgrade pyarrow"
      ],
      "metadata": {
        "id": "7pkOSai0z_sT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9fa6986-f278-4fb1-852e-3144080aef7e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (15.0.0)\n",
            "Collecting pyarrow\n",
            "  Using cached pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.25.2)\n",
            "Installing collected packages: pyarrow\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 15.0.0\n",
            "    Uninstalling pyarrow-15.0.0:\n",
            "      Successfully uninstalled pyarrow-15.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyarrow-17.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BitsAndBytesConfig, GemmaTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "zQgtL4U1tFq7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model, token=os.environ['HF_TOKEN'])\n",
        "model = AutoModelForCausalLM.from_pretrained(model,\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             device_map={\"\":0},\n",
        "                                             token=os.environ['HF_TOKEN'])"
      ],
      "metadata": {
        "id": "ZEqBbXts9gby",
        "outputId": "840fec6e-5a38-422c-942c-9fa1e12bc318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-74e9c7257e53>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HF_TOKEN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m model = AutoModelForCausalLM.from_pretrained(model,\n\u001b[1;32m      3\u001b[0m                                              \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                              \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                              token=os.environ['HF_TOKEN'])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset(\"training data set\")\n",
        "data = data.map(lambda samples: tokenizer(samples[\"item\"]), batched=True)"
      ],
      "metadata": {
        "id": "M16qHd3ivOXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xml_urls = [\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_B0034',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_M31201',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_K319005',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_Z0252',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_I14033',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_I14034',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_I742126',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_K137087',\n",
        "\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_I739105',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_I751501',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_K116500',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_K338002',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_K090504',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_R0063',\n",
        "\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_J64001',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_J64750',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_K112149',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_K116201',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_K125100',\n",
        "    'https://parts.igem.org/cgi/xml/part.cgi?part=BBa_K131017'\n",
        "    # ... more URLs\n",
        "]"
      ],
      "metadata": {
        "id": "lEOcS5osrn6J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make a dataframe out of URLs\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "data = []\n",
        "\n",
        "for url in xml_urls:\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        xml_content = response.text\n",
        "        root = ET.fromstring(xml_content)\n",
        "        # Extract relevant data from the XML and append it to the 'data' list\n",
        "        for item in root.findall('.//part'):\n",
        "            data.append({\n",
        "                'type': item.find('part_type').text,\n",
        "                'name': item.find('part_name').text,\n",
        "                'results': item.find('part_results').text,\n",
        "                'description': item.find('part_short_desc').text,\n",
        "                })\n",
        "            subpart = item.find('.//sequences')\n",
        "            if subpart is not None:\n",
        "                # Similarly, find 'seq_data' directly under 'sequences'\n",
        "                for sequence in subpart.findall('./seq_data'):\n",
        "                    data.append({\"sequence\": sequence.text})\n",
        "            subpart = item.find('.//categories')\n",
        "            if subpart is not None:\n",
        "                # Similarly, find 'seq_data' directly under 'sequences'\n",
        "                for sequence in subpart.findall('./category'):\n",
        "                    data.append({\"category\": sequence.text})\n",
        "    else:\n",
        "        print(f\"Failed to fetch XML data from {url}\")"
      ],
      "metadata": {
        "id": "Rn9ap3NYtxlQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "0LHQIEhHt5Ru"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward fill missing values\n",
        "df_filled = df.fillna(method='ffill')\n",
        "\n",
        "df_unique = df_filled.drop_duplicates(subset=['type', 'name'])\n",
        "print(df_unique)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZzyyVno5eGz",
        "outputId": "16b827bd-49c8-473d-91d5-2900b7380ec8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          type         name results  \\\n",
            "0          RBS    BBa_B0034   Works   \n",
            "2   Regulatory   BBa_M31201   Works   \n",
            "4   Regulatory  BBa_K319005   Works   \n",
            "6   Regulatory    BBa_Z0252    None   \n",
            "8   Regulatory   BBa_I14033    None   \n",
            "10  Regulatory   BBa_I14034    None   \n",
            "12  Regulatory  BBa_I742126    None   \n",
            "14  Regulatory  BBa_K137087    None   \n",
            "16  Regulatory  BBa_I739105    None   \n",
            "18  Regulatory  BBa_I751501   Works   \n",
            "20  Regulatory  BBa_K116500   Works   \n",
            "22  Regulatory  BBa_K338002   Works   \n",
            "24  Regulatory  BBa_K090504   Works   \n",
            "26  Regulatory    BBa_R0063    None   \n",
            "28  Regulatory   BBa_J64001   Works   \n",
            "30  Regulatory   BBa_J64750    None   \n",
            "32  Regulatory  BBa_K112149    None   \n",
            "34  Regulatory  BBa_K116201    None   \n",
            "36  Regulatory  BBa_K125100    None   \n",
            "38  Regulatory  BBa_K131017    None   \n",
            "\n",
            "                                          description  \\\n",
            "0        RBS (Elowitz 1999) -- defines RBS efficiency   \n",
            "2   Yeast CLB1 promoter region, G2/M cell cycle sp...   \n",
            "4                      yeast mid-length ADH1 promoter   \n",
            "6                    T7 weak binding and processivity   \n",
            "8                                              P(Cat)   \n",
            "10                                             P(Kat)   \n",
            "12               Reverse lambda cI-regulated promoter   \n",
            "14  optimized (TA) repeat constitutive promoter wi...   \n",
            "16  Double Promoter (LuxR/HSL, positive / cI, nega...   \n",
            "18                            plux-cI hybrid promoter   \n",
            "20  OmpF promoter that is activated or repressesed...   \n",
            "22  K338001+R0011: Heat Shock Promoter + LacI Regu...   \n",
            "24         Gram-Positive Strong Constitutive Promoter   \n",
            "26      Promoter (luxR & HSL regulated -- lux pL)<br>   \n",
            "28                       psicA from <I>Salmonella</I>   \n",
            "30  SPI-1 TTSS secretion-linked promoter from <I>S...   \n",
            "32   PmgtCB Magnesium promoter from <I>Salmonella</I>   \n",
            "34               ureD promoter from <I>P mirabilis<I>   \n",
            "36  nir promoter from <I>Synechocystis</I> sp. PCC...   \n",
            "38                  p_qrr4 from <I>Vibrio harveyi</I>   \n",
            "\n",
            "                                             sequence  \n",
            "0                                                 NaN  \n",
            "2                                      aaagaggagaaa\\n  \n",
            "4   atttttacagcatcatttatgggtatcctgcaagttaggtgcggaa...  \n",
            "6   gatatccttttgttgtttccgggtgtacaatatggacttcctcttt...  \n",
            "8               ataattaattgaactcactaaagggagaccacagc\\n  \n",
            "10           ggcacgtaagaggttccaactttcaccataatgaaaca\\n  \n",
            "12    cattattgcaattaataaacaactaacggacaattctacctaaca\\n  \n",
            "14  gcaaccattatcaccgccagaggtaaaatagtcaacacgcacggtg...  \n",
            "16              ttgacaatatatatatatatatatataatgctagc\\n  \n",
            "18  acctgtaggatcgtacaggtttacgcaagaaaatggtttgttatag...  \n",
            "20  acctgtaggatcgtacaggtttacgtaacaccgtgcgtgttgatgc...  \n",
            "22  gacggtgttcacaaagttccttaaattttacttttggttacatatt...  \n",
            "24  ccgaggtccttgttgcgaagattgatgacaatgtgagtgcttccct...  \n",
            "26  atttttaaagtatgtatacaaatgatgaataaattttggcgatata...  \n",
            "28  acctgtacgatcctacaggtgcttatgttaagtaattgtattccca...  \n",
            "30  ccacaagaaaacgaggtacggcattgagccgcgtaaggcagtagcg...  \n",
            "32  ccacaagaaaacgaggtacggcattgagccgcgtaaggcagtagcg...  \n",
            "34  tacggcggcaatcaggacgtttagcatcccttttctggtggaaccc...  \n",
            "36  tacggcggcaatcaggacgtttagcatcccttttctggtggaaccc...  \n",
            "38  gctaaatgcgtaaactgcatatgccttcgctgagtgtaatttacgt...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#export to CSV\n",
        "df_unique.to_csv('igem_parts1.csv', index=False)  # Set index=False to avoid saving row numbers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3h6mIoLBKOf",
        "outputId": "a4a795b2-f99f-4e4b-afce-4df32974f5b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame exported to 'igem_parts.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as etree\n",
        "\n",
        "# parse the XML content\n",
        "root = etree.fromstring(xml_content)\n",
        "for element in root:\n",
        "    # access element tag, attributes, and text content\n",
        "    print(element.tag, element.attrib, element.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUdaYQRlqtc-",
        "outputId": "634c9bda-0d8f-453f-f22a-ffd616dbee4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part_list {} \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1cd2ded675f442d3bc71fc728fb4842c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_756b89517be441c499e14ff764854d03",
              "IPY_MODEL_d86abe6fba874423bee1676cb369a276",
              "IPY_MODEL_2f84caa194ef47e1ab7a07f70a83e20b"
            ],
            "layout": "IPY_MODEL_f01430ac583145bc930db97b944d466c"
          }
        },
        "756b89517be441c499e14ff764854d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_123c0d2fa9ae467ea9a5ea219e18a4ca",
            "placeholder": "​",
            "style": "IPY_MODEL_6eb3a7d891064d94b8aeb9e4ee8f383f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d86abe6fba874423bee1676cb369a276": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f0892e1666c4041a12f34817e8ae9b3",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b07f43b0cab242ec9dcc5102913e8c63",
            "value": 4
          }
        },
        "2f84caa194ef47e1ab7a07f70a83e20b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_314e96af884049d9b4777c305cb785fc",
            "placeholder": "​",
            "style": "IPY_MODEL_113250a32c2c4120b3282becc5d7c1a3",
            "value": " 4/4 [01:21&lt;00:00, 17.96s/it]"
          }
        },
        "f01430ac583145bc930db97b944d466c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "123c0d2fa9ae467ea9a5ea219e18a4ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eb3a7d891064d94b8aeb9e4ee8f383f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f0892e1666c4041a12f34817e8ae9b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b07f43b0cab242ec9dcc5102913e8c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "314e96af884049d9b4777c305cb785fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "113250a32c2c4120b3282becc5d7c1a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}